{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "248b1de2-9883-4039-9976-771a9a16fea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu121\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "836f6415-84be-43b1-b576-f998f982c33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_pytorch import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ce8093-5a83-4c91-b363-9918290e4d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from linformer import Linformer\n",
    "from PIL import Image\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm.notebook import tqdm\n",
    "from vit_pytorch.efficient import ViT\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cba03f57-3585-4a87-a622-6c588b32241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61d7c786-d42f-482e-bf57-d6c8a09ac07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "batch_size = 32 \n",
    "epochs = 20\n",
    "# lr = 3e-5\n",
    "lr = 0.001\n",
    "gamma = 0.7\n",
    "seed = 142\n",
    "IMG_SIZE = 256\n",
    "patch_size = 16\n",
    "num_classes = 2\n",
    "step_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e866a0f8-7c5e-4b30-9a2b-624f2d80041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),  # Resize all images to the same size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0577619b-b8f0-4a6f-a2e6-3fb8909c715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets with the transform\n",
    "train_ds = torchvision.datasets.ImageFolder(r'C:\\Mine\\Work\\Uzi\\Signature_Verification\\train', transform=transform)\n",
    "valid_ds = torchvision.datasets.ImageFolder(r'C:\\Mine\\Work\\Uzi\\Signature_Verification\\validation', transform=transform)\n",
    "test_ds = torchvision.datasets.ImageFolder(r'C:\\Mine\\Work\\Uzi\\Signature_Verification\\test', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c56924f-bb3c-4107-8408-75385a6ce3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders:\n",
    "train_loader = data.DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=1, pin_memory = True)\n",
    "val_loader = data.DataLoader(valid_ds, batch_size=batch_size, shuffle=True,  num_workers=1, pin_memory = True)\n",
    "test_loader  = data.DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e9de2f-3c8a-4fd4-936f-f36b0fd953c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training device:\n",
    "device = 'cuda'\n",
    "\n",
    "# # Linear Transformer:\n",
    "# efficient_transformer = Linformer(dim=128, seq_len=64+1, depth=12, heads=8, k=64)\n",
    "\n",
    "# Vision Transformer Model: \n",
    "model = ViT(dim=128, image_size=128, patch_size=patch_size, num_classes=num_classes, transformer=efficient_transformer, channels=3).to(device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# # Optimizer\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Learning Rate Scheduler for Optimizer:\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adef40a-cb71-4a01-9ba7-f565a8fbccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficient_transformer = Linformer(\n",
    "    dim=128,\n",
    "    seq_len=(IMG_SIZE // patch_size) ** 2 + 1,  # Adjusting seq_len to match the number of patches\n",
    "    depth=12,\n",
    "    heads=8,\n",
    "    k=64\n",
    ")\n",
    "\n",
    "model = ViT(\n",
    "    dim=128,\n",
    "    image_size=IMG_SIZE,\n",
    "    patch_size=patch_size,\n",
    "    num_classes=num_classes,\n",
    "    transformer=efficient_transformer,\n",
    "    channels=3\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01087e84-6041-4ee3-ab18-4cafba321dbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ViT.__init__() got an unexpected keyword argument 'heads'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m emb_dropout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Initialize the Vision Transformer\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mViT\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# depth = depth,\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheads\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mheads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmlp_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmlp_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43memb_dropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43memb_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\n\u001b[0;32m     24\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mTypeError\u001b[0m: ViT.__init__() got an unexpected keyword argument 'heads'"
     ]
    }
   ],
   "source": [
    "# Define your model parameters\n",
    "IMG_SIZE = 256\n",
    "patch_size = 32\n",
    "num_classes = 2  # Adjust according to your dataset\n",
    "dim = 1024\n",
    "depth = 6\n",
    "heads = 16\n",
    "mlp_dim = 2048\n",
    "dropout = 0.1\n",
    "emb_dropout = 0.1\n",
    "\n",
    "# Initialize the Vision Transformer\n",
    "model = ViT(\n",
    "    image_size = IMG_SIZE,\n",
    "    patch_size = patch_size,\n",
    "    num_classes = num_classes,\n",
    "    dim = dim,\n",
    "    # depth = depth,\n",
    "    heads = heads,\n",
    "    mlp_dim = mlp_dim,\n",
    "    dropout = dropout,\n",
    "    emb_dropout = emb_dropout,\n",
    "    channels = 3\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d145ec3-49d0-431f-8f88-95f103b2dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "lr = 3e-5  # Set your learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07fd0fb-906f-48aa-99db-5968f1a09234",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf57d40-8226-4c14-80c6-458ac0fdac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee887f4-0928-4a1a-b4b1-a78adbbf0239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32822557-de99-4204-b4bf-ee78e17be284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler\n",
    "gamma = 0.7  # Set your gamma\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5679135f-7a79-4f81-83d3-0eeebf4a6e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training:\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "\n",
    "    for data, label in tqdm(train_loader):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        with autocast(True):\n",
    "            output = model(data)\n",
    "            loss = criterion(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = (output.argmax(dim=1) == label).float().mean()\n",
    "        epoch_accuracy += acc / len(train_loader)\n",
    "        epoch_loss += loss / len(train_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    # Clear cache here after training phase and before validation\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        epoch_val_accuracy = 0\n",
    "        epoch_val_loss = 0\n",
    "\n",
    "        for data, label in val_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            val_output = model(data)\n",
    "            val_loss = criterion(val_output, label)\n",
    "\n",
    "            acc = (val_output.argmax(dim=1) == label).float().mean()\n",
    "            epoch_val_accuracy += acc / len(val_loader)\n",
    "            epoch_val_loss += val_loss / len(val_loader)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34422c4f-af2a-4b10-874a-d8f84f6fe595",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
